# ADVANCED FINE-TUNING REPO

The repo is split across branches. This branch is for Direct Preference Optimization (DPO).

Please post questions under "Issues"

## Getting started

- DPO.ipynb allows you to run dpo training on a model. It requires a DPO dataset.
- You can purchase a Llama 2 formatted dataset [here](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo).

## Helper scripts

- hh_rlhf_dpo.ipynb allows for dataset generation if you wish to prepare datasets yourself.
